<html>
<!-- 2025 Gordon Williams, gw@pur3.co.uk

This controls a machine made out of lego with Jolt.js for control, to sort Lego pieces based on Tensorflow.js and in-browser learning

This is the code exactly as was used for the Espruino demo table at WebAI Summit 2025

-->
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.5">
    <title>Lego Sorter</title>
    <style>
      button { font-size:200%; padding:10px;margin: 20px; }
    </style>
  </head>
  <body>
    <img src="img/joltjs.png"/>&nbsp;&nbsp;&nbsp;<img src="img/espruino.png"/><br/>

    <video id="v" width="720" height="720" style="display:none" muted></video>
    <canvas id="c" width="720" height="720"></canvas><br/>
    <button id="connect">Connect</button>
    <button id="feed">Feed</button>
    <button id="stop">STOP</button>
    <br/>
    <button id="train_0"></button>
    <button id="train_1"></button>
    <button id="train_2"></button>
    <button id="train_3"></button>
    <div id="status">Loading...</div>
    <br/>
    <div id="found">
       <canvas id="thumbcanvas" width="300" height="340"></canvas>
       <div style="float:right"><button id="retrain_0"></button><br/>
       <button id="retrain_1"></button><br/>
       <button id="retrain_2"></button><br/>
       <button id="retrain_3"></button></div>
    </div>

    <p><a href="https://github.com/gfwilliams/gfwilliams.github.io/blob/master/experiments/lego_sorter.html">GitHub</a></p>

<script src="https://www.espruino.com/js/uart.js" type="text/javascript"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js" type="text/javascript"></script>
  <script>
var video, width, height, ctx, thumbctx;
var constraints = {video: {/* facingMode: 'environment', */aspectRatio: {ideal: 1}, width:720, height:720 }, audio:false};
const MIDX = 360;
const BOX = {
  drop : { x : MIDX, y: 500, size : 300 }, // detects when
};
const MOBILE_NET_INPUT_WIDTH = 224;
const MOBILE_NET_INPUT_HEIGHT = 224;
const STOP_DATA_GATHER = -1;
const CLASS_NAMES = ["Empty", "Square", "Round", "Different"];
let mobilenet = undefined;
let model = undefined;
let lastPredictionImage; // last image used for prediction
let thumbPredictionImage; // image used for prediction that made the thumbnail
let gatherDataState = STOP_DATA_GATHER;
let videoPlaying = false;
let videoLastFrameTime = 0;
let trainingDataInputs = [];
let trainingDataOutputs = [];
let examplesCount = [];
let predict = false;
let predictedClass = "", predictedClassNumber = -1;
let predictedClassStableFor = 0; // time prediction is stable for
let state = "";
let stateChangeTimeout = undefined;

function setStatus(txt) {
  if (!txt.startsWith("Prediction")) console.log(txt);
  document.getElementById("status").innerText = txt;
}

function logProgress(epoch, logs) {
  setStatus('Data for epoch ' + epoch, logs);
}

/**
 * Loads the MobileNet model and warms it up so ready for use.
 **/
async function loadMobileNetFeatureModel() {
  const URL =
    'https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v3_small_100_224/feature_vector/5/default/1';

  mobilenet = await tf.loadGraphModel(URL, {fromTFHub: true});
  setStatus('MobileNet v3 loaded successfully!');

  // Warm up the model by passing zeros through it once.
  tf.tidy(function () {
    let answer = mobilenet.predict(tf.zeros([1, MOBILE_NET_INPUT_HEIGHT, MOBILE_NET_INPUT_WIDTH, 3]));
    console.log(answer.shape);
  });
}


function initialize() {
  // Call the function immediately to start loading.
  loadMobileNetFeatureModel();
  // init model
  model = tf.sequential();
  model.add(tf.layers.dense({inputShape: [1024], units: 128, activation: 'relu'}));
  model.add(tf.layers.dense({units: CLASS_NAMES.length, activation: 'softmax'}));
  model.summary();
  // Compile the model with the defined optimizer and specify a loss function to use.
  model.compile({
    // Adam changes the learning rate over time which is useful.
    optimizer: 'adam',
    // Use the correct loss function. If 2 classes of data, must use binaryCrossentropy.
    // Else categoricalCrossentropy is used if more than 2 classes.
    loss: (CLASS_NAMES.length === 2) ? 'binaryCrossentropy': 'categoricalCrossentropy',
    // As this is a classification problem you can record accuracy in the logs too!
    metrics: ['accuracy']
  });

  // The source video.
  video = document.getElementById("v");
  width = video.width;
  height = video.height;

  // The target canvas.
  var canvas = document.getElementById("c");
  ctx = canvas.getContext("2d", { willReadFrequently:true });
  var thumbcanvas = document.getElementById("thumbcanvas");
  thumbctx = thumbcanvas.getContext("2d", { willReadFrequently:true });

  // The bpm meter
  bpm = document.getElementById("bpm");

  // Get the webcam's stream.
  if (navigator.mediaDevices.getUserMedia) {
    navigator.mediaDevices.getUserMedia(constraints)
      .then(startStream)
      .catch(console.error)
  } else {
    navigator.getUserMedia(constraints, startStream, function () {});
  }
}

function getVideoImage() {
  let box = BOX.drop;
  let img = ctx.getImageData(box.x-box.size/2, box.y-box.size/2, box.size, box.size);
  return img;
}

function getVideoTensor(img) {
  let videoFrameAsTensor = tf.browser.fromPixels(img);
  let resizedTensorFrame = tf.image.resizeBilinear(videoFrameAsTensor, [MOBILE_NET_INPUT_HEIGHT, MOBILE_NET_INPUT_WIDTH], true);
  /*for (var i=0;i<img.data.length;i++) img.data[i] |= 0x1F; // debugging
  ctx.putImageData(img, box.x-box.size/2, box.y-box.size/2);*/
  return resizedTensorFrame.div(255);
}

function addNewTrainingData(expectedClass, img) {
  let imageFeatures = tf.tidy(function() {
    return mobilenet.predict(getVideoTensor(img).expandDims()).squeeze();
  });

  trainingDataInputs.push(imageFeatures);
  trainingDataOutputs.push(expectedClass);

  // Intialize array index element if currently undefined.
  if (examplesCount[expectedClass] === undefined) {
    examplesCount[expectedClass] = 0;
  }
  examplesCount[expectedClass]++;
  let status = '';
  for (let n = 0; n < CLASS_NAMES.length; n++) {
    status += CLASS_NAMES[n] + ' data count: ' + examplesCount[n] + '. ';
  }
  setStatus(status);

  trainAndPredict();
}

function dataGather() {
  if (videoPlaying && gatherDataState !== STOP_DATA_GATHER) {
    let img = getVideoImage();
    addNewTrainingData(gatherDataState, img);
    gatherDataState = STOP_DATA_GATHER; // only one frame!
  }
}

async function trainAndPredict() {
  predict = false;
  tf.util.shuffleCombo(trainingDataInputs, trainingDataOutputs);
  let outputsAsTensor = tf.tensor1d(trainingDataOutputs, 'int32');
  let oneHotOutputs = tf.oneHot(outputsAsTensor, CLASS_NAMES.length);
  let inputsAsTensor = tf.stack(trainingDataInputs);

  let results = await model.fit(inputsAsTensor, oneHotOutputs, {shuffle: true, batchSize: 5, epochs: 10,
      callbacks: {onEpochEnd: logProgress} });

  outputsAsTensor.dispose();
  oneHotOutputs.dispose();
  inputsAsTensor.dispose();
  predict = true;
}

function makePrediction() {
  if (predict) {
    tf.tidy(function() {
      let img = getVideoImage();
      lastPredictionImage = img;
      let resizedTensorFrame = getVideoTensor(img)
      let imageFeatures = mobilenet.predict(resizedTensorFrame.expandDims());
      let prediction = model.predict(imageFeatures).squeeze();
      let highestIndex = prediction.argMax().arraySync();
      let predictionArray = prediction.arraySync();
      if (predictedClass == CLASS_NAMES[highestIndex])
        predictedClassStableFor++;
      else {
        predictedClassStableFor = 0;
        predictedClass = CLASS_NAMES[highestIndex];
      }
      predictedClassNumber = highestIndex;
      setStatus('Prediction: ' + CLASS_NAMES[highestIndex] + ' with ' + Math.floor(predictionArray[highestIndex] * 100) + '% confidence (stable '+predictedClassStableFor+')');
    });
  }
}

function startStream(stream) {
  videoPlaying = true;
  video.srcObject = stream;
  video.play();
  // Ready! Let's start drawing.
  requestAnimationFrame(draw);
}

function showRetrainThumb() {
  thumbPredictionImage = lastPredictionImage;
  thumbctx.clearRect(0, 0, 300,340);
  thumbctx.putImageData(thumbPredictionImage, 0,0);
  thumbctx.font = "24px sans";
  thumbctx.textAlign = "center";
  thumbctx.textBaseline = "middle";
  thumbctx.fillText(predictedClass+"?", 150, 320);
}

function draw() {
  let videoFrameTime = video.currentTime;
  if (videoLastFrameTime == videoFrameTime) {
    requestAnimationFrame(draw); // skip duplicate video frames
    return;
  }
  videoLastFrameTime = videoFrameTime;
  try {
    ctx.drawImage(video, 0, 0, width, height);
  } catch (e) {
    // The video may not be ready, yet.
    return null;
  }
  dataGather();
  makePrediction();
  if (state=="FEED") {
    if (predictedClass!="Empty" && predictedClassStableFor>5) {
      setStatus(`Found ${predictedClass} - feeding!`);
      predict = false;
      // show retrain message
      showRetrainThumb();
      // if we found something, feed it and then dump it off
      state = "DUMP"
      joltSetFeeder(0);
      let feedTime = 200 + predictedClassNumber*1500; // 1.5sec per amount

      joltFeedAndPush(feedTime).then(() => {
        setStatus(`Feed and push complete.`);
      });
    }
  }
  function drawBox(box) {
    ctx.rect(box.x - box.size/2, box.y - box.size/2,
            box.size, box.size);
  }
  var midx = 360;
  ctx.strokeStyle = "red";
  ctx.beginPath();
  drawBox(BOX.drop);
  ctx.stroke();
  var img = ctx.getImageData(0, 0, width, height);
  requestAnimationFrame(draw);
}

// ---------------------------------------------------------------------------------------------------------
// Push the lego
function joltPush() {
  if (UART.isConnected())
    UART.write("\x10digitalWrite([H5,H4],1);setTimeout(()=>digitalWrite([H5,H4],0),1000);\n");
  return new Promise(resolve => setTimeout(resolve,1000));
}
// Set the lego feeder
function joltSetFeeder(on) {
  if (UART.isConnected())
    UART.write(`\x10digitalWrite([H2,H3],${on?1:0})\n`)
}
// Set whether the belt is running
function joltSetBelt(on) {
  if (UART.isConnected())
    UART.write(`\x10digitalWrite([H7,H6],${on?1:0})\n`)
}
function joltFeedAndPush(feedTime) {
  UART.write(`\x10digitalWrite([H7,H6],1);if (${feedTime}) setTimeout(function() {
  digitalWrite([H7,H6],0)
  digitalWrite([H5,H4],1);setTimeout(()=>digitalWrite([H5,H4],0),1000);
}, ${feedTime});\n`);
  return new Promise(resolve => {
    stateChangeTimeout = setTimeout(function() {
      stateChangeTimeout = undefined;
      startFeeding();
      resolve();
    }, feedTime+1000);
  });
}
// Set the lego feeder
function startFeeding() {
  if (stateChangeTimeout) {
    clearTimeout(stateChangeTimeout);
    stateChangeTimeout = undefined;
  }
  joltSetFeeder(1);
  predict = true;
  state = "FEED";
}
// Stop everything
function joltStop(on) {
  if (UART.isConnected())
    UART.write(`\x10digitalWrite([H7,H6,H5,H4,H3,H2,H1,H0],0);clearInterval();\n`)
}
// ---------------------------------------------------------------------------------------------------------
document.getElementById("connect").addEventListener("click", function() {
  UART.write("reset()\n").then(function() {
    return new Promise(r => setTimeout(r, 500));
  }).then(function() {
    return UART.write("NRF.setConnectionInterval(7.5)\n");
  }).then(function() {
    setStatus("Connected to Jolt.js");
  });
});
document.getElementById("feed").addEventListener("click", function() {
  startFeeding();
});
document.getElementById("stop").addEventListener("click", function() {
  setStatus(`Stopping!`);
  if (stateChangeTimeout) {
    clearTimeout(stateChangeTimeout);
    stateChangeTimeout = undefined;
  }
  state = "";
  joltStop();
});
for (let i=0;i<CLASS_NAMES.length;i++) {
  let btn = document.getElementById("train_"+i);
  btn.innerText = CLASS_NAMES[i];
  btn.addEventListener("click", function() {
    gatherDataState = i;
  });
  btn = document.getElementById("retrain_"+i);
  btn.innerText = "Train as "+CLASS_NAMES[i];
  btn.addEventListener("click", function() {
    addNewTrainingData(i, thumbPredictionImage);
  });
}
addEventListener("load", initialize);
  </script>
  </body>
</html>